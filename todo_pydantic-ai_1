TODO: Pydantic AI Refactor Phase 1

Project Overview
This is the first phase of modernizing the podcast generation system's LLM interactions by replacing manual JSON parsing with Pydantic AI agents. The goal is to eliminate error-prone JSON parsing code while maintaining 100% backward compatibility.

Design Decisions Made

Core Architecture Decision
- Target Method: Refactor generate_text_async in GeminiService to use Pydantic AI internally
- Backward Compatibility: Add optional result_type parameter that defaults to str (current behavior)
- Migration Strategy: Gradual conversion - methods can opt into structured outputs one at a time

Method Signature Design
```python
async def generate_text_async(
    self, 
    prompt: str, 
    timeout_seconds: int = 180,
    result_type: Optional[Type] = None  # NEW: Defaults to str for backward compatibility
) -> Union[str, Any]:
```

Authentication Strategy
- Continue using existing GEMINI_API_KEY environment variable
- Maintain current Google Gemini API integration pattern

Retry Strategy
- Remove existing @retry decorator from generate_text_async
- Let Pydantic AI handle retries natively
- This eliminates duplicate retry logic

Feature Flag Strategy
- Simple boolean environment variable: USE_PYDANTIC_AI=true/false
- Default to false (existing implementation) for production safety
- No percentage rollout needed for this small-scale project

Observability Strategy
- Add Logfire instrumentation to generate_text_async (single point of LLM interaction)
- Track specific metrics: error rates, response time, input text length, retry attempt counts, JSON parsing failure rates
- Preserve all existing logging for MCP server integration

Testing Strategy
- Focus on end-to-end functionality testing of new implementation
- No A/B comparison testing needed
- Validate that structured outputs match expected Pydantic models

Phase 1 Scope

Primary Goal
Replace generate_text_async internal implementation with Pydantic AI while maintaining identical external interface.

Secondary Goal
Convert analyze_source_text_async to use result_type=SourceAnalysis as proof of concept, eliminating all JSON parsing code in that method.

Out of Scope for Phase 1
- Other methods (research_persona_async, generate_podcast_outline_async, etc.) - these remain unchanged
- Workflow orchestration changes
- Agent hierarchy or nested agent patterns
- Changes to method signatures other than generate_text_async

Implementation Plan

Phase 1.1: Environment Setup
☐ Add pydantic-ai dependency to project
☐ Add logfire dependency to project
☐ Configure Logfire project and obtain API keys
☐ Set up development environment with new dependencies
☐ Create USE_PYDANTIC_AI environment variable mechanism

Phase 1.2: Core Implementation
☐ Research Pydantic AI GeminiModel configuration with API key authentication
☐ Design agent initialization strategy within GeminiService class
☐ Implement new generate_text_async method using Pydantic AI
☐ Add feature flag logic to switch between implementations
☐ Remove @retry decorator from existing implementation
☐ Preserve existing implementation as _legacy_generate_text_async fallback

Phase 1.3: Observability Integration
☐ Configure Logfire integration in the project
☐ Design observability schema for target metrics
☐ Instrument new Pydantic AI implementation in generate_text_async
☐ Ensure all existing logging statements are preserved (MCP server dependency)
☐ Test observability data collection in development

Phase 1.4: Proof of Concept
☐ Convert analyze_source_text_async to use result_type=SourceAnalysis
☐ Remove all JSON parsing code from analyze_source_text_async:
  - Eliminate _clean_json_string_from_markdown() calls
  - Remove manual json.loads() and error handling
  - Remove ValidationError exception handling
  - Remove regex-based JSON extraction logic
☐ Validate that SourceAnalysis objects are identical to current output

Phase 1.5: Testing & Validation
☐ Create comprehensive test suite for new generate_text_async implementation
☐ Test both string output (default behavior) and structured output modes
☐ Add integration tests with real Gemini API calls
☐ Test error scenarios and edge cases
☐ Validate feature flag switching works correctly
☐ Test that all existing methods continue working unchanged

Phase 1.6: Production Readiness
☐ Configure production Logfire environment
☐ Test feature flag in development environment thoroughly
☐ Document new configuration options and usage patterns
☐ Create monitoring dashboards for observability metrics
☐ Prepare rollback procedures and testing checklist
☐ Update inline code documentation

Success Criteria

Functional Requirements
☐ generate_text_async maintains identical behavior when called without result_type
☐ analyze_source_text_async produces identical SourceAnalysis objects as current implementation
☐ All other methods (research_persona_async, generate_dialogue_async, etc.) continue working unchanged
☐ Feature flag allows seamless switching between old and new implementations
☐ No regression in error handling or timeout behavior

Code Quality Requirements
☐ All JSON parsing code eliminated from analyze_source_text_async
☐ Existing MCP server logging preserved
☐ Type hints properly maintained
☐ No breaking changes to any method signatures

Observability Requirements
☐ Logfire provides actionable insights into LLM performance
☐ All target metrics (error rates, response time, input length, retry counts, JSON parsing failures) are collected
☐ Monitoring dashboards functional in development environment

Production Requirements
☐ Feature flag ready for production deployment
☐ Rollback procedures tested and documented
☐ Configuration properly externalized via environment variables

Risk Mitigation

Backward Compatibility Risks
- Mitigation: Extensive testing of default behavior (string returns)
- Fallback: Feature flag allows immediate rollback to existing implementation

Authentication Risks
- Mitigation: Use existing GEMINI_API_KEY approach, no authentication changes
- Validation: Test API connectivity in development before production deployment

Performance Risks
- Mitigation: Monitor response times via Logfire observability
- Validation: Compare performance between old and new implementations

Integration Risks
- Mitigation: Preserve all existing logging for MCP server integration
- Validation: Test that workflow orchestration remains unchanged

Future Phases (Out of Scope)
- Phase 2: Convert research_persona_async to use result_type=PersonaResearch
- Phase 3: Convert generate_podcast_outline_async to use result_type=PodcastOutline
- Phase 4: Evaluate agent hierarchy patterns for workflow orchestration
- Phase 5: Remove legacy implementation after all methods converted

Key Dependencies
- Pydantic AI library and Gemini model support
- Logfire observability platform
- Existing GEMINI_API_KEY authentication
- Current SourceAnalysis Pydantic model compatibility

Documentation References
- Pydantic AI Documentation
- GeminiModel Configuration
- Logfire Python SDK
- Pydantic AI with Logfire Integration
- Retry

Addendum to TODO: Pydantic AI Refactor Phase 1

Additional Design Decisions Made

Agent Initialization Strategy
- Decision: Constructor initialization (single agent instance)
- Rationale: Standardized settings across methods, lower performance overhead, simpler implementation for Phase 1
- Implementation: Create single self.pydantic_agent in GeminiService.__init__()

Standardized Agent Configuration
- Timeout: 300 seconds for all calls (ignore timeout_seconds parameter in Pydantic AI mode)
- Retry Strategy: Use Pydantic AI defaults (remove custom @retry decorator)
- Model: gemini-1.5-pro
- Behavior: Identical across all methods for Phase 1
- Backward Compatibility: Keep timeout_seconds parameter in method signature for legacy mode

Error Mapping Strategy
- Decision: Option 2 - Map Pydantic AI exceptions to existing exception types
- Exception Mapping:
  - ValidationError → Pass through unchanged
  - Timeout exceptions → DeadlineExceeded
  - All other Pydantic AI exceptions → LLMProcessingError
- Debug Logging: Log original Pydantic AI exceptions for troubleshooting
- Rationale: Zero breaking changes to existing error handling code

Logfire Setup Status
- Status: Logfire already configured 
- Impact: Phase 1.3 simplified - focus on instrumentation rather than setup

Updated Implementation Tasks

Phase 1.2: Core Implementation - Additional Tasks
 Research Pydantic AI exception types and hierarchy
 Implement exception mapping logic in generate_text_async
 Add debug logging for original Pydantic AI exceptions
 Remove @retry decorator from generate_text_async
 Implement standardized 300s timeout (ignore parameter in Pydantic AI mode)

Phase 1.3: Logfire Integration - Streamlined
 Configure Logfire project → Already done
 Design observability schema for metrics
 Instrument generate_text_async with Logfire spans
 Add comparison logging between legacy and Pydantic AI implementations

Phase 1.5: Testing & Validation - Additional
 Test that existing error handling patterns remain unchanged
 Validate timeout behavior (300s standardization)
 Test exception mapping for all error scenarios
 Verify retry behavior matches expectations with Pydantic AI defaults
